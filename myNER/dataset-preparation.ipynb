{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.128956Z",
     "start_time": "2026-01-04T07:29:26.965516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Sequence, ClassLabel, Value, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ],
   "id": "f2d0d57dcd6f8953",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing Functions",
   "id": "7ea8040e65675dcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.133835Z",
     "start_time": "2026-01-04T07:29:28.129429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_break_pattern():\n",
    "    \"\"\"Creates and returns the regular expression pattern for Myanmar syllable breaking.\"\"\"\n",
    "    my_consonant = r\"က-အ\"\n",
    "    # en_char = r\"a-zA-Z0-9\"\n",
    "\n",
    "    other_char = r\"ဣဤဥဦဧဩဪဿ၌၍၏၀-၉၊။\"\n",
    "    subscript_symbol = r'္'\n",
    "    a_that = r'်'\n",
    "\n",
    "    # Regular expression pattern for Myanmar syllable breaking\n",
    "    return re.compile(\n",
    "        r\"((?<!\" + subscript_symbol + r\")[\" + my_consonant + r\"]\"\n",
    "        r\"(?![\"\n",
    "        + a_that + subscript_symbol + r\"])\"\n",
    "        + r\"|[\" + other_char + r\"])\"\n",
    "    )\n",
    "\n",
    "def break_syllables(line, break_pattern, separator):\n",
    "    \"\"\"Applie\n",
    "    s syllable breaking rules to a line.\"\"\"\n",
    "    line = re.sub(r'\\s+', ' ', line.strip()) # Normalize space\n",
    "    segmented_line = break_pattern.sub(separator + r\"\\1\", line)\n",
    "\n",
    "    # Remove the leading delimiter if it exists\n",
    "    if segmented_line.startswith(separator):\n",
    "        segmented_line = segmented_line[len(separator):]\n",
    "\n",
    "    # Replace delimiter+space+delimiter with a single space\n",
    "    double_delimiter = separator + \" \" + separator\n",
    "    segmented_line = segmented_line.replace(double_delimiter, \" \")\n",
    "\n",
    "    return segmented_line\n",
    "\n",
    "break_pattern = create_break_pattern()\n",
    "\n",
    "def mm_split(text):\n",
    "    text = text.strip()\n",
    "    seperator = \"|X|\"\n",
    "\n",
    "    result = break_syllables(text, break_pattern, seperator)\n",
    "    result = result.split(seperator)\n",
    "\n",
    "    return result"
   ],
   "id": "c6b22e604eaabf33",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.138087Z",
     "start_time": "2026-01-04T07:29:28.134171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bioes_to_bio(tag):\n",
    "    if tag.startswith('E-'):\n",
    "        return 'I-' + tag[2:]\n",
    "    elif tag.startswith('S-'):\n",
    "        return 'B-' + tag[2:]\n",
    "    return tag\n",
    "\n",
    "def expand_tag_for_syllables(tag, count):\n",
    "    \"\"\"Expand a single tag for multiple syllables with correct BIO format.\"\"\"\n",
    "    if count == 1:\n",
    "        return [tag]\n",
    "\n",
    "    if tag.startswith('B-'):\n",
    "        # B-X followed by I-X for remaining syllables\n",
    "        return [tag] + ['I-' + tag[2:]] * (count - 1)\n",
    "    elif tag.startswith('I-'):\n",
    "        # All I-X\n",
    "        return [tag] * count\n",
    "    else:\n",
    "        # O tags stay O\n",
    "        return [tag] * count\n",
    "\n",
    "def load_conll(filepath):\n",
    "    sentences = []\n",
    "    current_sentence = {'tokens': [], 'pos': [], 'ner': []}\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                if current_sentence['tokens']:\n",
    "                    sentences.append({\n",
    "                        'tokens': current_sentence['tokens'],\n",
    "                        'pos': current_sentence['pos'],\n",
    "                        'ner': current_sentence['ner']\n",
    "                    })\n",
    "                    current_sentence = {'tokens': [], 'pos': [], 'ner': []}\n",
    "            elif not line.startswith('#'):\n",
    "                parts = line.split('\\t')\n",
    "\n",
    "                word = parts[0]\n",
    "                words = mm_split(word)\n",
    "                word_count = len(words)\n",
    "\n",
    "                pos = [parts[1]] * word_count\n",
    "\n",
    "                # Convert BIOES to BIO, then expand correctly\n",
    "                bio_tag = bioes_to_bio(parts[2])\n",
    "                ner_tags = expand_tag_for_syllables(bio_tag, word_count)\n",
    "                ner = ner_tags\n",
    "\n",
    "                word = words\n",
    "\n",
    "                current_sentence['tokens'].extend(word)\n",
    "                current_sentence['pos'].extend(pos)\n",
    "                current_sentence['ner'].extend(ner)\n",
    "\n",
    "    if current_sentence['tokens']:\n",
    "        sentences.append({\n",
    "            'tokens': current_sentence['tokens'],\n",
    "            'pos': current_sentence['pos'],\n",
    "            'ner': current_sentence['ner']\n",
    "        })\n",
    "\n",
    "    return sentences"
   ],
   "id": "f147cc2616bb106d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.140700Z",
     "start_time": "2026-01-04T07:29:28.138328Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9fa32d3d7e0c6fbf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T06:45:48.707313Z",
     "start_time": "2026-01-04T06:45:48.698600Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Create HuggingFace Format Dataset",
   "id": "a8843b900b90c4ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.566012Z",
     "start_time": "2026-01-04T07:29:28.140928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and convert to DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = load_conll('myNER-7tags_ver.1.0.conll')\n"
   ],
   "id": "805dc2a22d46c796",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.572816Z",
     "start_time": "2026-01-04T07:29:28.566449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tags = set()\n",
    "for row in data:\n",
    "    all_tags.update(row[\"ner\"])"
   ],
   "id": "86d835cddbddfb3c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.576270Z",
     "start_time": "2026-01-04T07:29:28.573095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sort_tags(tags):\n",
    "    def tag_key(tag):\n",
    "        if tag == 'O':\n",
    "            return ('ZZZ', 0)  # Use string 'ZZZ' to put 'O' at the end\n",
    "        prefix, entity = tag.split('-')\n",
    "        prefix_order = {'B': 0, 'I': 1}\n",
    "        return (entity, prefix_order.get(prefix, 2))\n",
    "\n",
    "    return sorted(tags, key=tag_key)\n",
    "\n",
    "tag_list = sort_tags(list(all_tags))\n",
    "tag2id = {tag: idx for idx, tag in enumerate(tag_list)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}"
   ],
   "id": "4282dda137e1aac0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.585570Z",
     "start_time": "2026-01-04T07:29:28.576462Z"
    }
   },
   "cell_type": "code",
   "source": "tag_list",
   "id": "b06e42920c122eaf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DATE',\n",
       " 'I-DATE',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-NUM',\n",
       " 'I-NUM',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-TIME',\n",
       " 'I-TIME',\n",
       " 'O']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.599363Z",
     "start_time": "2026-01-04T07:29:28.586203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for row in data:\n",
    "    row[\"ner_tags\"] = [tag2id[ner_tag] for ner_tag in row['ner']]"
   ],
   "id": "f4b388c25fecd703",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.602368Z",
     "start_time": "2026-01-04T07:29:28.599755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=tag_list)),\n",
    "})"
   ],
   "id": "15cfe59a62db7b4d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.622439Z",
     "start_time": "2026-01-04T07:29:28.602560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(data)\n",
    "before_size = df.shape[0]\n",
    "\n",
    "# Convert list to string for deduplication\n",
    "df['tokens_str'] = df['tokens'].apply(lambda x: ''.join(x))\n",
    "df = df.drop_duplicates(subset='tokens_str')\n",
    "df = df.drop(columns=['tokens_str', 'pos', 'ner'])\n",
    "after_size = df.shape[0]\n",
    "\n",
    "# Simple split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "print(\"Total row removed:\", before_size - after_size)"
   ],
   "id": "7996cb588df92c4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12825\n",
      "Test: 3207\n",
      "Total row removed: 572\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.663038Z",
     "start_time": "2026-01-04T07:29:28.623445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert back to dataset\n",
    "deduped_ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"test\": Dataset.from_pandas(test_df, preserve_index=False),\n",
    "})"
   ],
   "id": "58b4a30c47850391",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.697773Z",
     "start_time": "2026-01-04T07:29:28.663337Z"
    }
   },
   "cell_type": "code",
   "source": "deduped_ds = deduped_ds.cast(features)",
   "id": "3e028cff85dfafd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12825 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d79f96937a4b48b390bc961bd18bc4c2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3207 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "812cb6739f0d49cd9dbd1f3d23413334"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.710842Z",
     "start_time": "2026-01-04T07:29:28.698334Z"
    }
   },
   "cell_type": "code",
   "source": "deduped_ds",
   "id": "52196c83b67143b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 12825\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3207\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:28.719833Z",
     "start_time": "2026-01-04T07:29:28.711594Z"
    }
   },
   "cell_type": "code",
   "source": "deduped_ds['train']",
   "id": "c83d59dbf9118d99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 12825\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upload to HuggingFace",
   "id": "cf46cabacd16b660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:37.382740Z",
     "start_time": "2026-01-04T07:29:28.730537Z"
    }
   },
   "cell_type": "code",
   "source": "deduped_ds.push_to_hub(\"chuuhtetnaing/myanmar-ner-dataset\", private=True)",
   "id": "ff0c1780234e109b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f18f0c72fbdd4b43a8aa5ceaa6c43ab9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a029dab083446b38002d3ab654b7e11"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4d5d6be01d64ab29e64de4a63d43676"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb761c56c8c94bc18ef194be6cd1e247"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc1c629bdca749a5b24e64a45af33862"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d30fb6034bbf46dfac6c999f5d74c13d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5da786e2c9c54b719ce1c6544414becb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5b6be6cfb144924b6b3d2734d6f78e5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/chuuhtetnaing/myanmar-ner-dataset/commit/1cd21f97fd84932b0860c018542f75d3ec755245', commit_message='Upload dataset', commit_description='', oid='1cd21f97fd84932b0860c018542f75d3ec755245', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/chuuhtetnaing/myanmar-ner-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='chuuhtetnaing/myanmar-ner-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T07:29:37.447520Z",
     "start_time": "2026-01-04T07:29:37.440579Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5d82ea2f6cd83aad",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
